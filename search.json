[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "",
    "text": "To Cadets:\nThis course, MA206, introduces you to the foundational principles of probability and statistics, emphasizing data literacy and inference. It begins with Block I, covering data types, visualization, and basic probability rules including counting and the behavior of random variables. Block II builds on this by exploring discrete and continuous distributions, the Central Limit Theorem, and tools for one-sample inference such as confidence intervals and hypothesis testing for proportions and means. Finally, Block III develops cadets’ ability to analyze relationships between variables through two-sample tests, linear regression, ANOVA, and goodness-of-fit testing. By the end of the course, cadets will be equipped to make sound, data-driven decisions grounded in statistical reasoning.\nSix Step Method\nA Note on Technology:\nIn this course the primary tool used for data analysis is R. Throughout this course you will implement techniques for summarizing, visualizing, and analyzing data. The primary focus of this course is not for you to become masters in coding, however building on skills learned in CY105 will help your analysis in understanding how to use information technology to demonstrate successful outcomes in this course.\nOf note, some of the functions we use in R require the package tidyverse, everytime you begin working in RStudio, the beginning code chunk should resemble:\n#&gt; echo:FALSE\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidyr)\nlibrary(patchwork)"
  },
  {
    "objectID": "index.html#types-of-data-sampling-and-bias",
    "href": "index.html#types-of-data-sampling-and-bias",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Types of Data, Sampling, and Bias",
    "text": "Types of Data, Sampling, and Bias\nFundamental to statistical analysis is understanding the types of data we encounter, the methods we use to collect them, and the potential sources of bias that can undermine the validity of our conclusions. We distinguish between categorical (qualitative) and quantitative (numerical) data. Categorical variables can further be broken down into nominal (color, baseball position, type of animal), ordinal (I had a very bad/somewhat bad/neutral/good/very good experience at CFT), and binary (Yes I passed Air Assault/No I did not). Quantitative data can also be broken down into either discrete or continuous. Understanding these distinctions is critical for selecting the correct tools for analysis and interpretation.\nThe two methods of sampling in this course will be through simple random sampling or convenience sampling. The difference in application is whether we can generalize our results to the larger population. As an example, say I do not have an exhaustive list of cadet ID numbers to randomly select from. Instead, I only can stand in Central Area after class at 1630 and survey the first 50 cadets that willingly take my survey. There is a certain sub-population I am probably missing (1st Reg, 4th Reg, Corps Squad, etc). This is a convenience sample. Instead, if I randomly select 50 C-Numbers from a complete list of the Corps obtained from the registrar, this would be a a true random sample of the Corps, whereas the former is what is known as selection bias.\nFinally, we explore the concept of bias in data collection. We identify common sources such as selection bias, response bias, and measurement bias, and discuss how poor sampling practices or flawed survey design can distort findings. This lesson sets the stage for the rest of the course by highlighting the importance of thoughtful data collection and critical evaluation of data sources.\nAs an example let us look at a dataset aggregated from over 50,000 diamonds:\n\n#&gt; echo: false\nset.seed(1991)\ndata(diamonds)\ndf &lt;- diamonds %&gt;% sample_n(size=1000)\ndf %&gt;% head\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1.02 Good      G     VS2      63.8  59    6080  6.34  6.27  4.02\n2  0.31 Ideal     F     VVS1     61.9  53.5   882  4.36  4.39  2.71\n3  0.6  Premium   D     SI2      61.3  61    1428  5.46  5.4   3.33\n4  0.41 Ideal     E     IF       62.1  54    1419  4.75  4.81  2.97\n5  0.72 Very Good H     VS1      62.2  54    2877  5.74  5.76  3.57\n6  1.2  Ideal     F     VS2      62.6  56    8486  6.78  6.73  4.23\n\n\nEach of the rows is an individual diamond, generally called an observation. Each of the columns are unique aspects measured for every observation, called variables. Variables are either categorical, qualitative aspects of each measurement, or quantitative, a numbered entry."
  },
  {
    "objectID": "index.html#exploratory-data-analysis",
    "href": "index.html#exploratory-data-analysis",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nUnderstanding, communicating, and interpreting your data is paramount to any initial data analysis project. These are done through numerous visualizations and summary statistics which we will learn to regularly implement when given any new dataset.\n\nOne Variable – Visualizations and Summary Statistics\nStarting with a variable-by-variable approach is a natural first step. This is done rapidly in R with the following few functions:\n\n#&gt; echo: false\np1 &lt;- diamonds %&gt;% sample_n(size=1000) %&gt;% ggplot(aes(x=carat)) + geom_histogram(bins=30) + theme_minimal()\np2 &lt;- diamonds %&gt;% sample_n(size=1000) %&gt;% ggplot(aes(x=depth)) + geom_histogram(bins=30) + theme_minimal()\np3 &lt;- diamonds %&gt;% sample_n(size=1000) %&gt;% ggplot(aes(x=price)) + geom_histogram(bins=30) + theme_minimal()\np1 | p2 | p3\n\n\n\n\n\n\n\n\nHistograms tell us where most of the values for a quantitative variable lie in its given distribution. We can determine skewness, a measure of how lopsided the data appear or if there are any asymmetries or tails.\n\n#&gt; echo: false\np4 &lt;- diamonds %&gt;% sample_n(size=1000) %&gt;% ggplot(aes(x=carat)) + geom_boxplot() + theme_minimal()\np5 &lt;- diamonds %&gt;% sample_n(size=1000) %&gt;% ggplot(aes(x=depth)) + geom_boxplot() + theme_minimal()\np6 &lt;- diamonds %&gt;% sample_n(size=1000) %&gt;% ggplot(aes(x=price)) + geom_boxplot() + theme_minimal()\np4 | p5 | p6\n\n\n\n\n\n\n\n\nSimilar to a histogram, a boxplot will tell us exactly where the median, 1st and 3rd quartiles, and outliers exist for any quantitative variable. The ‘whiskers’ are determined by \\(1.5 \\times IQR\\) where the inter-quartile range is the \\(3rd - 1st\\) quartiles.\n\n#&gt; echo: false\ndf_summary &lt;- df %&gt;%\n  summarise(across(where(is.numeric), list(\n    Mean = ~mean(.x, na.rm = TRUE),\n    Median = ~median(.x, na.rm = TRUE),\n    SD = ~sd(.x, na.rm = TRUE),\n    Var = ~var(.x, na.rm = TRUE),\n    Min = ~min(.x, na.rm = TRUE),\n    Max = ~max(.x, na.rm = TRUE)\n  ), .names = \"{.col}_{.fn}\")) %&gt;%\n  pivot_longer(everything(), names_to = c(\"Variable\", \"stat\"), names_sep = \"_\") %&gt;%\n  pivot_wider(names_from = stat, values_from = value)\n\nprint(df_summary)\n\n# A tibble: 7 × 7\n  Variable     Mean  Median       SD          Var    Min      Max\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 carat       0.790    0.7     0.462        0.213   0.23     3.24\n2 depth      61.8     61.9     1.40         1.97   56.3     68.9 \n3 table      57.5     57       2.26         5.11   50       66   \n4 price    3874.    2387    3913.    15308046.    337    18692   \n5 x           5.72     5.68    1.10         1.21    3.88     9.44\n6 y           5.72     5.68    1.09         1.20    3.9      9.4 \n7 z           3.53     3.52    0.677        0.458   2.39     5.85\n\n\nThe above are the predominant statistics you want to discern for every quantitative variable in your dataset. The benchmark location statistics are the mean, median, max, and min, while the standard deviation and variance are measures of how spread out the data are relative to one another.\n\\[\n\\begin{align}\n\\text{Sample Mean: } \\ \\ & \\bar{X} = \\frac{1}{n}\\sum_{i=1}^n x_i \\\\\n\\text{Sample Variance: } \\ \\ &  S^2 = \\frac{1}{n-1}\\sum_{i=1}^n ( x_i - \\bar{X}  )^2 \\\\\n\\text{Sample Standard Deviation:} \\ \\ & s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n ( x_i - \\bar{X} )^2}\n\\end{align}\n\\]\nNote: the standard deviation is the square root of the variance\n\n\nTwo Variables – Visualizations and Summary Statistics\n\n#&gt; echo: false\np7 &lt;- df %&gt;% ggplot(aes(x=carat, y=price, color=clarity)) + geom_point() + theme_minimal()\np8 &lt;- df %&gt;% ggplot(aes(x=table, y=price, color=cut)) + geom_point() + theme_minimal()\np9 &lt;- df %&gt;% ggplot(aes(x=depth, y=price, color=color)) + geom_point() + theme_minimal()\np7 | p8 |p9\n\n\n\n\n\n\n\n\nA scatterplot is the main tool to visualize and identify a relationship between two quantitative variables. Oftentimes, coloring each observation by another categorical variable is a way to maximize effectiveness of a single plot, as you are encoding more information within the same space.\n\n#&gt; echo: false\np10 &lt;- df %&gt;%\n  group_by(cut) %&gt;%\n  summarise(avg_price = mean(price)) %&gt;%\n  ggplot(aes(x = cut, y = avg_price)) +\n  geom_bar(stat = \"identity\", fill = \"pink\") +\n  labs(\n    title = \"Average Price by Diamond Cut Quality\",\n    x = \"Diamond Cut\",\n    y = \"Average Price ($)\"\n  ) +\n  theme_minimal()\np11 &lt;- df %&gt;%\n  group_by(color) %&gt;%\n  summarise(avg_price = mean(price)) %&gt;%\n  ggplot(aes(x = color, y = avg_price)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(\n    title = \"Average Price by Diamond Color Rating\",\n    x = \"Diamond Color\",\n    y = \"Average Price ($)\"\n  ) +\n  theme_minimal()\np12 &lt;- df %&gt;%\n  group_by(clarity) %&gt;%\n  summarise(avg_price = mean(price)) %&gt;%\n  ggplot(aes(x = clarity, y = avg_price)) +\n  geom_bar(stat = \"identity\", fill = \"magenta\") +\n  labs(\n    title = \"Average Price by Diamond Clarity\",\n    x = \"Diamond Clarity\",\n    y = \"Average Price ($)\"\n  ) +\n  theme_minimal()\np10 | p11 | p12\n\n\n\n\n\n\n\n\n\n#&gt; echo: false\ncor_df &lt;- df %&gt;%\n  select(where(is.numeric)) %&gt;%\n  cor(use = \"pairwise.complete.obs\")\n\nprint(cor_df)\n\n             carat        depth      table       price          x           y\ncarat  1.000000000 -0.003309893  0.2218141  0.91021728  0.9771650  0.97682747\ndepth -0.003309893  1.000000000 -0.3160834 -0.02418736 -0.0637861 -0.06707873\ntable  0.221814133 -0.316083433  1.0000000  0.16644576  0.2396640  0.23527498\nprice  0.910217282 -0.024187362  0.1664458  1.00000000  0.8763495  0.87899268\nx      0.977165035 -0.063786104  0.2396640  0.87634955  1.0000000  0.99885498\ny      0.976827473 -0.067078728  0.2352750  0.87899268  0.9988550  1.00000000\nz      0.976811196  0.058609118  0.1951975  0.87535333  0.9913793  0.99109238\n               z\ncarat 0.97681120\ndepth 0.05860912\ntable 0.19519747\nprice 0.87535333\nx     0.99137925\ny     0.99109238\nz     1.00000000\n\n\nCorrelation is the only multivariate summary statistic we will be using in this course, used to describe how two variables tend to move in tandem with one another. A perfect linear association evokes a correlation of 1, the opposite being a perfect negative association with a correlation of -1. No association is implied by a correlation near 0.\nMathematically: &gt; Definition &gt; For any two variables X,Y, the correlation of X and Y are: \\[\nr = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2\\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\]"
  },
  {
    "objectID": "index.html#probability",
    "href": "index.html#probability",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Probability",
    "text": "Probability\n\nSample Space and Random Experiment (\\(\\Omega\\))\nA random experiment is a process that produces an outcome which cannot be predicted with certainty in advance. It must be well-defined, have more than one possible outcome, and be repeatable under similar conditions. Each performance of the experiment results in a single outcome from the sample space. The sample space is the set of all possible outcomes of a random experiment.\n\n\nDefinition:\n\nThe sample space is the set of all possible outcomes of a random experiment, denoted \\(\\Omega\\).\nAn event is a subset of the sample space. It can represent one or more outcomes.\nIf all outcomes in \\(\\Omega\\) are equally likely, then for any event \\(A\\):\n\n\n\\[\n\\mathbb{P}(A) = \\frac{\\text{Number of outcomes in } A}{\\text{Total outcomes in } \\Omega}\n\\]\n\nExamples:\n\nTossing a coin once: \\(\\Omega = {\\text{Heads}, \\text{Tails}}\\)\nRolling a 6-sided die: \\(\\Omega = {1, 2, 3, 4, 5, 6}\\)\nLetter grade in MA206: \\(\\Omega = {A, B, C, D, F}\\)\nNumber of emails received in an hour: \\(\\Omega = {0, 1, 2, \\dots}\\)\n\n\n\n\nProbability Measure (\\(\\mathbb{P}\\))\nA probability measure is a rule, denoted \\(\\mathbb{P}\\), that assigns a number between 0 and 1 to every event in a collection of events (called a sigma-algebra, denoted ). These probabilities must follow three key rules, known as the axioms of probability.\n\n\n\n\n\n\n\nAxioms of Probability\n\n\n\n\nNon-Negativity\nFor any event \\(A\\), the probability is never negative:\n\\[\n\\mathbb{P}(A) \\geq 0\n\\]\nNormalization\nThe probability of one of the events happening over the entire sample space is 1:\n\\[\n\\mathbb{P}(\\Omega) = 1\n\\]\nAdditivity (for disjoint events)\nIf events \\(A_1, A_2, A_3, \\dots\\) are mutually exclusive (no overlap), then the probability that any one of them occurs is the sum of their individual probabilities:\n\\[\n\\mathbb{P}\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\mathbb{P}(A_1) + \\mathbb{P}(A_2) + \\mathbb{P}(A_3) + \\cdots\n\\]\n\n\n\n\nExample (Simple):\nLet \\(A\\), \\(B\\), and \\(C\\) be outcomes when rolling a die:\n\n\\(A = \\{1\\}\\), $B = {3} $, \\(C = \\{5\\}\\)\n\nThese are disjoint events (they don’t overlap).\n\nThen: \\[\n\\mathbb{P}(A \\cup B \\cup C) = \\mathbb{P}(A) + \\mathbb{P}(B) + \\mathbb{P}(C) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{2}\n\\]\n\nThese three rules form the mathematical foundation of all probability calculations — everything else builds on them.\n\n\n\n\n\n\n\nAxioms of Probability\n\n\n\n\nThe complement of an event ( A ), written ( A^c ), consists of all outcomes in ( ) that are not in ( A ):\n\n\\[\n\\mathbb{P}(A) + \\mathbb{P}(A^c) = 1\n\\]\n\nThe intersection $ A B $ consists of outcomes where both \\(A\\) and \\(B\\) occur.\nThe union \\(A \\cup B\\) consists of outcomes where either \\(A\\), \\(B\\), or both occur:\n\n\\[\n\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\n\\]\n\nTwo events \\(A\\) and \\(B\\) are disjoint (mutually exclusive) if they cannot both occur:\n\n\\[\nA \\cap B = \\varnothing \\quad \\text{and} \\quad \\mathbb{P}(A \\cap B) = 0\n\\]\n\n\n\n\nConditional Probability\nFor events \\(A\\) and \\(B\\) with $0 &lt; P(B) $, the conditional probability of \\(A\\) given \\(B\\) is:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nExample: One card is drawn from a standard deck.\nLet \\(A\\): card is a Queen, and \\(B\\): card is a face card.\nFind \\(P(A)\\), \\(P(B)\\), and \\(P(A \\mid B)\\).\n\n\n\nLaw of Total Probability\n\nDefinition:\nIf \\(E_1, \\dots, E_n\\) is a partition of the sample space (mutually exclusive and exhaustive), then for any event \\(A\\):\n\n\\[\nP(A) = \\sum_{i=1}^{n} P(E_i) P(A \\mid E_i)\n\\]\nExample:\nA fair die is rolled. Let event A: “an even number is rolled”.\nLet:\n- \\(E_1\\): roll is 1 or 2\n- \\(E_2\\): roll is 3 or 4\n- \\(E_3\\): roll is 5 or 6\n\nFind:\n- \\(P(A \\mid E_1)\\), \\(P(A \\mid E_2)\\), \\(P(A \\mid E_3)\\)\n- Then use the Law of Total Probability to find \\(P(A)\\)\n\n\n\nBayes’ Theorem\n\nDefinition:\nIf \\(E_1, \\dots, E_n\\) is a partition of the sample space and \\(P(A) &gt; 0\\), then:\n\n\\[\nP(E_k \\mid A) = \\frac{P(A \\mid E_k) P(E_k)}{\\sum_{i=1}^n P(A \\mid E_i) P(E_i)}\n\\]\nExample:\nTwo urns:\n- Urn 1: 1 red, 1 blue\n- Urn 2: 3 red, 1 blue\nPick an urn at random, then draw one ball.\nIf the ball is red, what is the probability it came from Urn 1?\n\n\n\nCounting Principles\nBefore we can begin a thorough treatment of probability, some concepts in counting are needed to identify four common situations. These arise depending on when things are “allowed” to repeat or the “order” items are chosen in matters. The ability to discern when these four situations arise is more than half the battle.\n\nOrdered with Replacement\nThink of the number of ways of choosing a 4-digit passcode on your phone.\nThe order of the numbers matters, and you are allowed to repeat the same number. So how many arrangements are there? Since repetition is allowed and order matters, there are 10 digits for each position, giving:\n\\[\n\\text{Ordered Arrangements with Replacement} = n^r = 10^4 = 10{,}000\n\\]\n\n\nOrdered without Replacement\nThink of the number of ways I can create a batting order from 9 position players.\nThe order still matters, but players cannot be repeated. This is a permutation — an ordered arrangement without replacement.\n\\[\n{}_nP_r = P(n, r) = \\frac{n!}{(n - r)!}\n\\]\nFor example, the number of ways to assign the first 3 batting positions from 9 players:\n\\[\n{}_9P_3 = \\frac{9!}{(9-3)!} = 9 \\times 8 \\times 7 = 504\n\\]\n\n\nUnordered without Replacement\nThink of how many ways you can choose 3 scoops of ice cream from 5 unique flavors without repeats.\nBecause order doesn’t matter and repeats aren’t allowed, we use combinations:\n\\[\n{}_nC_k = \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\]\n\n\nUnordered with Replacement\nThink of how many different combinations of 3 scoop ice cream cones you can make with 5 unique flavors while allowing repeats.\nThis is the trickiest scenario. The order doesn’t matter, and repetition is allowed. The formula is:\n\\[\n\\text{Unordered Arrangements with Replacement} = \\binom{r+n-1}{r} = \\frac{(r+n-1)!}{r!(n-1)!}\n\\]\nExample: choosing 3 scoops from 5 flavors (with repeats):\n\\[\n\\binom{3+5-1}{3} = \\binom{7}{3} = 35\n\\]\nThis can be understood using the stars and bars method: selecting \\(r\\) scoops with \\(n-1\\) dividers. Imagine representing each scoop as a ★ (star) and using vertical bars | to separate flavor types. To choose \\(r\\) scoops from \\(n\\) flavors, you need \\(r\\) stars (for the scoops) and \\(n - 1\\) bars (to divide them into \\(n\\) categories). For example, if \\(r = 3\\) scoops and \\(n = 5\\) flavors, you arrange 3 stars and 4 bars in a row. One possible arrangement is ★ | ★★ | | |, which represents 1 scoop of flavor 1, 2 scoops of flavor 2, and 0 scoops of flavors 3, 4, and 5. The number of such arrangements is given by the combination formula \\(\\binom{r + n - 1}{r}\\), since you are choosing positions for the \\(r\\) indistinguishable stars among the \\(r + n - 1\\) total positions (stars and bars combined).\nNote: The above section may seem like it came out of nowhere, that is okay. A fundamentally difficulty inherent in probability is finding the sample space or event space due to finding the various different combinations/permutations sequences of different possibilities. To elaborate consider the next example: \n\nExample: No Matching Pairs in a Shoe Sample {.unnumbered}\nA closet contains \\(n\\) pairs of shoes (so \\(2n\\) total shoes). If \\(2r\\) shoes are chosen at random (where \\(2r &lt; n\\)), what is the probability that no matching pair is selected?\nWe are selecting \\(2r\\) shoes such that no left and right shoe from the same pair are both chosen.\nStrategy: 1. First choose \\(2r\\) distinct pairs from the \\(n\\) available — there are \\(\\binom{n}{2r}\\) ways to do this.\n2. From each of these \\(2r\\) selected pairs, choose only one shoe (either left or right) — there are \\(2^{2r}\\) ways to do this.\n3. The total number of ways to choose any \\(2r\\) shoes out of \\(2n\\) is \\(\\binom{2n}{2r}\\).\nSo, the desired probability is:\n\\[\n\\mathbb{P}(\\text{No matching pair}) = \\frac{\\binom{n}{2r} \\cdot 2^{2r}}{\\binom{2n}{2r}}\n\\]"
  },
  {
    "objectID": "index.html#random-variables-expectation-and-variance",
    "href": "index.html#random-variables-expectation-and-variance",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Random Variables, Expectation, and Variance",
    "text": "Random Variables, Expectation, and Variance\n\nDefinition (Random Variable):\nA random variable is a mapping that assigns a real number to every outcome in the sample space:\n\\[\nX: \\Omega \\rightarrow \\mathbb{R}\n\\]\n\n\nDefinition (Cumulative Distribution Function):\nA cumulative distribution function (CDF) is a function \\(F_X: \\mathbb{R} \\rightarrow [0,1]\\) defined by:\n\\[\nF_X(x) = \\mathbb{P}(X \\le x)\n\\]"
  },
  {
    "objectID": "index.html#discrete-random-variables",
    "href": "index.html#discrete-random-variables",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\n\nDefinition (Discrete Random Variable):\nA discrete random variable is one that takes countably many values in \\(\\mathbb{R}\\).\nIts probability mass function is given by:\n\\[\nf_X(x) = \\mathbb{P}(X = x)\n\\]\n\n\nDefinition (Expectation):\nThe expected value (mean) of a discrete random variable \\(X\\) is:\n\\[\n\\mathbb{E}[X] = \\sum_x x \\cdot \\mathbb{P}(X = x)\n\\]\n\n\nDefinition (Variance):\nThe variance of a discrete random variable \\(X\\) is:\n\\[\n\\mathrm{Var}(X) = \\mathbb{E}\\left[(X - \\mathbb{E}[X])^2\\right] = \\sum_x (x - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}(X = x)\n\\]\n\n\nBinomial Distribution:\n\nLet \\(X \\sim \\text{Binomial}(n, p)\\) where \\(n \\in \\mathbb{N}\\) and \\(0 &lt; p &lt; 1\\).\n\nProbability Mass Function (PMF):\n\\[\n\\mathbb{P}(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k}, \\quad \\text{for } k = 0, 1, \\dots, n\n\\]\nCumulative Distribution Function (CDF):\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = \\sum_{k=0}^{\\lfloor x \\rfloor} \\binom{n}{k} p^k (1 - p)^{n - k}\n\\]\nExpected Value:\n\\[\n\\mathbb{E}[X] = np\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = np(1 - p)\n\\]\n\n\n\n\nGeometric Distribution:\n\nLet \\(X \\sim \\text{Geometric}(p)\\) be the number of trials until the first success (including the success), where \\(0 &lt; p &lt; 1\\).\n\nProbability Mass Function (PMF):\n\\[\n\\mathbb{P}(X = k) = (1 - p)^{k - 1} p, \\quad \\text{for } k = 1, 2, 3, \\dots\n\\]\nCumulative Distribution Function (CDF):\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = 1 - (1 - p)^{\\lfloor x \\rfloor}\n\\]\nExpected Value:\n\\[\n\\mathbb{E}[X] = \\frac{1}{p}\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = \\frac{1 - p}{p^2}\n\\]"
  },
  {
    "objectID": "index.html#continuous-random-variables",
    "href": "index.html#continuous-random-variables",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\nCentral Limit Theorem (CLT)\nThe Central Limit Theorem (CLT) is one of the most important results in statistics. It states that the sampling distribution of the sample mean \\(\\bar{X}\\) becomes approximately normal as the sample size \\(n\\) increases, regardless of the shape of the population distribution (provided it has finite mean and variance).\nSpecifically, if \\(X_1, X_2, \\dots, X_n\\) are i.i.d. random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then:\n\\[\n\\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\xrightarrow{d} \\mathcal{N}(0,1) \\quad \\text{as } n \\to \\infty\n\\]\nThis justifies the widespread use of the normal distribution to approximate sample means in practice.\n\n\nCredit: The New York Times\n\n\n\nNormal Distribution\n\nNormal Distribution\nLet \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), where \\(\\mu \\in \\mathbb{R}\\) and \\(\\sigma &gt; 0\\).\n\nProbability Density Function (PDF):\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right), \\quad x \\in \\mathbb{R}\n\\]\nCumulative Distribution Function (CDF):\nThere is no closed-form expression, but it is denoted as:\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = \\Phi\\left( \\frac{x - \\mu}{\\sigma} \\right)\n\\]\nwhere \\(\\Phi\\) is the standard normal CDF.\nExpected Value:\n\\[\n\\mathbb{E}[X] = \\mu\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = \\sigma^2\n\\]\n\n\n\n\nExponential Distribution\n\nExponential Distribution\nLet \\(X \\sim \\text{Exponential}(\\lambda)\\) with \\(\\lambda &gt; 0\\).\n\nProbability Density Function (PDF):\n\\[\nf(x) = \\lambda e^{-\\lambda x}, \\quad x \\ge 0\n\\]\nCumulative Distribution Function (CDF):\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = 1 - e^{-\\lambda x}, \\quad x \\ge 0\n\\]\nExpected Value:\n\\[\n\\mathbb{E}[X] = \\frac{1}{\\lambda}\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = \\frac{1}{\\lambda^2}\n\\]"
  },
  {
    "objectID": "index.html#confidence-intervals",
    "href": "index.html#confidence-intervals",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nA confidence interval (CI) gives a range of plausible values for a population parameter based on a sample statistic. The general structure of any confidence interval is:\n\\[\n\\text{point estimate} \\ \\pm \\ \\text{margin of error}\n\\]\nMore specifically, for large samples or when the sampling distribution of the estimate is approximately normal:\n\\[\n\\text{CI} = \\hat{\\theta} \\ \\pm \\ z^* \\cdot \\text{SE}(\\hat{\\theta})\n\\]\nWhere:\n\n\\(\\hat{\\theta}\\) is the point estimate (e.g., \\(\\bar{x}\\) for the mean, \\(\\hat{p}\\) for a proportion)\n\\(z^*\\) is the critical value from the standard normal distribution (e.g., 1.96 for 95% confidence)\n\\(\\text{SE}(\\hat{\\theta})\\) is the standard error of the estimate\n\nThis structure applies to many common settings:\n\nCI for a population mean: \\[\n\\bar{x} \\pm z^* \\cdot \\frac{s}{\\sqrt{n}}\n\\]\nCI for a population proportion: \\[\n\\hat{p} \\pm z^* \\cdot \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\n\\]\n\n\nInterpretation:\n\n“We are 95% confident that the true population parameter lies within this interval.”\n\nThis does not mean there’s a 95% probability the parameter is in the interval — rather, it means that 95% of all intervals computed from repeated samples in this manner would contain the true parameter."
  },
  {
    "objectID": "index.html#one-sample-hypothesis-testing",
    "href": "index.html#one-sample-hypothesis-testing",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "One Sample Hypothesis Testing",
    "text": "One Sample Hypothesis Testing\nHypothesis testing is a formal method for making inferences about a population using sample data. The whole test aspect is questioning if the statistic your sample shows is significantly different than a certain value in question. You have two underlying premises, referred to as the null and alternative hypotheses. The null hypothesis assumes that there is no difference: the statistic from your value is the same as the value you are testing. The alternative conflicts the null and says they are different. The process involves:\n\nState the null and alternative hypotheses. There are three different variants to create your hypotheses statements depending on what the question being asked entails.\n\n\nGreater than Alternative Hypothesis\n\n\\[\nH_0: Parameter = Value \\ in \\ Question \\\\\nH_A: Parameter &gt; Value \\ in \\ Question\n\\]\nThe entire inference aspect of hypothesis testing is that you are using your sample statistic, a tangible aspect of your data, to make an argument about the population parameter, an entity that is unknown to you. This is why the hypotheses are written in terms of the parameter. You are testing whether an aspect or parameter about the population is greater than a benchmark value decided by you in advance.\n\nLess than Alternative Hypothesis\n\n\\[\nH_0: Parameter = Value \\ in \\ Question \\\\\nH_A: Parameter &lt; Value \\ in \\ Question \\\\\n\\]\nVery similarly, the less than hypothesis is also a one-sided hypothesis test in that you are only testing one side of the value, abeit this time if the population parameter is less than the tested value.\n\nNot equal to Alternative Hypothesis\n\n\\[\nH_0: Parameter = Value \\ in \\ Question \\\\\nH_A: Parameter \\neq Value \\ in \\ Question \\\\\n\\]\nThis is the only two-sided hypothesis test, denoted with the not equal to alternative hypothesis. Both sides must be accounted for in this test, and therefore as we will see shortly require more evidence for significance.\n\nChoose a significance level \\(\\alpha\\).\n\nThis is the threshold you will also decide to inform your certainty in your conclusions. As seen previously, unless you are finding the probability that something will happen in the entire sample space (which happens probability 1); ie, there are no absolutes. Therefore there is always a chance that your conclusion will be wrong. Here is where you decide how “often” you are willing to be wrong. Is it 1% of the time? 5% of the time? Think of the significance level as choosing the percentage of the time you are willing to be wrong in your conclusion, (I know this sounds weird). A common \\(\\alpha\\) is 5%.\n\nCompute the test statistic.\n\nCompute the relevant summary statistic. In this course you will either be calculating a sample proportion, denoted \\(\\hat{p}\\) if the variable of interest is categorical or the sample mean \\(\\bar{X}\\) if quantitative.\n\nStandardize the test statistic.\n\nThis step transforms your statistic so it can be treated as a random variable from a named distribution. For proportions this will be a Normal Random Variable, however if quantitative it will be a t-distributed random variable.\n\n\nCredit: 365 Data Science\n\nDetermine the p-value.\n\nOnce we have the standardized statistic, it can be treated as either a Standard Normal Random Variable or Student’s-t Random Variable. This is where the alternative hypotheses come in to play to determine the p-value: the probability that if the null hypothesis is indeed true you choose to make an argument supporting the alternative. To find the probability of a certain event happening in a continuous random variable, you integrate the probability density function with the limits of integration being the range of values the random variable could take. Both the Standard Normal and Student’s-t are continuous, so depending on your alternative hypothesis, your p-value is calculated by either of the following:\n\\[\n\\begin{align}\n\\text{Greater Hypothesis} &&&& \\text{Less than Hypothesis} &&&& \\text{Two Sided Hypothesis} \\\\\n\\mathbb{P}(X&gt;z) = \\int_z^\\infty f_X(x) dx &&&& \\mathbb{P}(X&lt;z) = \\int_{-\\infty}^z f_X(x)dx &&&& \\mathbb{P}(X&gt;z) = \\int_{|z|}^\\infty f_X(x)dx + \\int_{-\\infty}^{-|z|} f_X(x)dx\n\\end{align}\n\\]\n\n#&gt; echo:false\n# Parameters\nmu &lt;- 0\nsigma &lt;- 1\n\n# Full normal curve\nx &lt;- seq(mu - 4*sigma, mu + 4*sigma, length.out = 1000)\ndf &lt;- data.frame(x = x, y = dnorm(x, mean = mu, sd = sigma))\n\n# Shade regions\nless_df &lt;- subset(df, x &lt;= -1.96)\nmore_df &lt;- subset(df, x &gt;= 1.96)\nboth_df &lt;- subset(df, x &lt;= -1.96 | x &gt;= 1.96)\n\n# Left tail\np_less &lt;- ggplot(df, aes(x = x, y = y)) +\n  geom_line(color = \"blue\", linewidth = 1.2) +\n  geom_area(data = less_df, aes(x = x, y = y), fill = \"lightblue\") +\n  geom_vline(xintercept = -1.96, linetype = \"dashed\", color = \"red\") +\n  theme_minimal()\n\n# Right tail\np_more &lt;- ggplot(df, aes(x = x, y = y)) +\n  geom_line(color = \"blue\", linewidth = 1.2) +\n  geom_area(data = more_df, aes(x = x, y = y), fill = \"lightblue\") +\n  geom_vline(xintercept = 1.96, linetype = \"dashed\", color = \"red\") +\n  theme_minimal()\n\n# Both tails\np_both &lt;- ggplot(df, aes(x = x, y = y)) +\n  geom_line(color = \"blue\", linewidth = 1.2) +\n  geom_area(data = less_df, aes(x = x, y = y), fill = \"lightblue\") +\n  geom_area(data = more_df, aes(x=x, y=y), fill = 'lightblue')+\n  geom_vline(xintercept = c(-1.96, 1.96), linetype = \"dashed\", color = \"red\") +\n  theme_minimal()\n\n# Show side by side\np_more | p_less | p_both\n\n\n\n\n\n\n\n\n\nMaking a conclusion based on comparison.\n\nOnce a p-value is obtained, reference it to the significance level chosen. If the p-value is greater than \\(\\alpha\\), you fail to reject the null hypothesis, if it is smaller, you reject the null hypothesis, and then state what that means in the context of the problem.\n\n\nSingle Proportion\nWe conduct inference on a population proportion \\(\\pi\\) relative to a hypothesized value \\(\\pi_0\\). The test statistic is:\n\\[\nz = \\frac{\\hat{p} - \\pi}{\\sqrt{\\frac{\\pi(1 - \\pi)}{n}}}\n\\]\n\nExample: Test if more than 40% of diamonds are “Ideal” cut\n\n# Null hypothesis: p = 0.20\npi &lt;- 0.4\nn &lt;- nrow(diamonds)\nphat &lt;- mean(diamonds$cut == \"Ideal\")\n\n# Test statistic and p-value\nz &lt;- (phat - pi) / sqrt(pi * (1 - pi) / n)\np_value &lt;- 1 - pnorm(z)\n\ncat(\"Z-statistic:\", round(z, 3), \"\\n\")\n\nZ-statistic: -0.22 \n\ncat(\"P-value:\", round(p_value, 4))\n\nP-value: 0.587\n\n\n\n\n\n\nSingle Mean\nWe conduct inference on a population mean \\(\\mu\\) relative to a hypothesized value \\(\\mu_0\\). The test statistic is:\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\nExample: Test if the average diamond price is $4000.\n\nmu0 &lt;- 4000\nx_bar &lt;- mean(diamonds$price)\ns &lt;- sd(diamonds$price)\nn &lt;- length(diamonds$price)\n\n# Test statistic and p-value\nt_stat &lt;- (x_bar - mu0) / (s / sqrt(n))\np_value &lt;- 2*(1 - pt(abs(t_stat), df = n - 1))\n\ncat(\"T-statistic:\", round(t_stat, 3), \"\\n\")\n\nT-statistic: -3.912 \n\ncat(\"P-value:\", round(p_value, 4))\n\nP-value: 1e-04\n\n\n\n\n\nExperimental Design: Power, Type I / Type II Error\nIn any hypothesis test, we face the possibility of making incorrect conclusions. These are formalized through Type I and Type II errors:\n\nType I Error (\\(\\alpha\\)): Rejecting the null hypothesis when it is actually true. This is controlled by the significance level of the test, often set to \\(\\alpha = 0.05\\).\nType II Error (\\(\\beta\\)): Failing to reject the null hypothesis when the alternative is actually true. This is harder to control and depends on the true parameter, sample size, and variance.\nPower of the Test: The probability of correctly rejecting the null hypothesis when the alternative is true: \\[\n\\text{Power} = 1 - \\beta\n\\]\n\nA powerful test detects meaningful effects and minimizes Type II error. Power increases when: - Sample size increases (\\(n \\uparrow\\)) - Effect size increases (true parameter is farther from null) - Variability decreases (standard deviation \\(\\downarrow\\)) - Significance level \\(\\alpha\\) increases (easier to reject null)\n\n\nVisual Summary\n\n\n\n\n\n\n\n\n\n\nThis diagram shows: - The blue curve is the null distribution (centered at 0). - The red dashed curve is the alternative distribution (shifted mean). - The dotted vertical line is the critical value (e.g., \\(z = 1.645\\) for \\(\\alpha = 0.05\\) in a one-sided test). - Area to the right of this cutoff under the null curve is \\(\\alpha\\). - Area to the left of this cutoff under the alternative curve is \\(\\beta\\). - The remaining area under the red curve (right tail) is power."
  },
  {
    "objectID": "index.html#two-sample-hypothesis-testing",
    "href": "index.html#two-sample-hypothesis-testing",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Two Sample Hypothesis Testing",
    "text": "Two Sample Hypothesis Testing\n\n\nDifference of Proportions\nWe compare two population proportions to determine whether there is a significant difference between them. The test statistic is:\n\\[\nz = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1 - \\hat{p}) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}}\n\\]\nWhere \\(\\hat{p}\\) is the pooled proportion.\n\nExample: Are “Ideal” cuts more common among Premium vs. Good color grades?\n\n# Subset to just Premium and Good clarity levels\ndf &lt;- diamonds %&gt;% filter(color %in% c(\"D\", \"E\"))\n\ntable(df$cut, df$color)\n\n           \n               D    E    F    G    H    I    J\n  Fair       163  224    0    0    0    0    0\n  Good       662  933    0    0    0    0    0\n  Very Good 1513 2400    0    0    0    0    0\n  Premium   1603 2337    0    0    0    0    0\n  Ideal     2834 3903    0    0    0    0    0\n\n# Create a binary outcome: Ideal or not\ndf &lt;- df %&gt;%\n  mutate(is_ideal = cut == \"Ideal\")\n\n# Proportions\np1 &lt;- mean(df$is_ideal[df$color == \"D\"])\np2 &lt;- mean(df$is_ideal[df$color == \"E\"])\nn1 &lt;- sum(df$color == \"D\")\nn2 &lt;- sum(df$color == \"E\")\n\n# Pooled proportion\nphat &lt;- (p1 * n1 + p2 * n2) / (n1 + n2)\n\n# Test statistic\nz &lt;- (p1 - p2) / sqrt(phat * (1 - phat) * (1 / n1 + 1 / n2))\np_value &lt;- 2 * (1 - pnorm(abs(z)))\n\ncat(\"Z-statistic:\", round(z, 3), \"\\n\")\n\nZ-statistic: 2.566 \n\ncat(\"P-value:\", round(p_value, 4))\n\nP-value: 0.0103\n\n\n\n\n\n\nMultiple Proportions (Chi-Square Test of Independence)\nUsed when comparing proportions across more than two groups.\n\nExample: Is cut independent of color?\n\ntbl &lt;- table(diamonds$cut, diamonds$color)\nchisq.test(tbl)\n\n\n    Pearson's Chi-squared test\n\ndata:  tbl\nX-squared = 310.32, df = 24, p-value &lt; 2.2e-16\n\n\nThis test checks whether the distribution of cut types is independent of the diamond color. A small p-value suggests a dependency.\n\n\n\n\nDifference of Means\nUsed to compare two independent sample means.\n\nExample: Is the average price different between “Ideal” and “Fair” cuts?\n\ndf &lt;- diamonds %&gt;% filter(cut %in% c(\"Ideal\", \"Fair\"))\n\nt.test(price ~ cut, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  price by cut\nt = 9.7484, df = 1894.8, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Fair and group Ideal is not equal to 0\n95 percent confidence interval:\n  719.9065 1082.5251\nsample estimates:\n mean in group Fair mean in group Ideal \n           4358.758            3457.542 \n\n\nThis performs a two-sample t-test, assuming unequal variances by default. The null hypothesis is that the means are equal.\n\n\n\n\nPaired Data\nIn paired designs, each observation in one group is paired with a related observation in the other. Since diamonds has no natural pairing, we’ll simulate a paired example.\n\nExample (Simulated): Price before and after resizing a set of diamonds\n\nset.seed(123)\n\n# Simulate paired prices: original and discounted\nn &lt;- 100\noriginal_price &lt;- sample(diamonds$price, n)\ndiscounted_price &lt;- original_price * runif(n, 0.85, 0.95)\n\nt.test(original_price, discounted_price, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  original_price and discounted_price\nt = 8.9496, df = 99, p-value = 2.132e-14\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 342.2012 537.1645\nsample estimates:\nmean difference \n       439.6829 \n\n\nThis tests whether the mean price before and after a simulated discount differs significantly."
  },
  {
    "objectID": "index.html#regression",
    "href": "index.html#regression",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Regression",
    "text": "Regression\n\nSimple Linear Regression\n\n\n\nMultiple Linear Regression\n\n\n\nGoodness of Fit\n\n\nANOVA"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]