[
  {
    "objectID": "course_guide.html",
    "href": "course_guide.html",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "",
    "text": "To Cadets:\nThis course, MA206, introduces you to the foundational principles of probability and statistics, emphasizing data literacy and inference. It begins with Block I, covering data types, visualization, and basic probability rules including counting and the behavior of random variables. Block II builds on this by exploring discrete and continuous distributions, the Central Limit Theorem, and tools for one-sample inference such as confidence intervals and hypothesis testing for proportions and means. Finally, Block III develops cadets’ ability to analyze relationships between variables through two-sample tests, linear regression, ANOVA, and goodness-of-fit testing. By the end of the course, cadets will be equipped to make sound, data-driven decisions grounded in statistical reasoning.\nSix Step Method\nA Note on Technology:\nIn this course the primary tool used for data analysis is R. Throughout this course you will implement techniques for summarizing, visualizing, and analyzing data. The primary focus of this course is not for you to become masters in coding, however building on skills learned in CY105 will help your analysis in understanding how to use information technology to demonstrate successful outcomes in this course.\nOf note, some of the functions we use in R require the package tidyverse, everytime you begin working in RStudio, the beginning code chunk should resemble:"
  },
  {
    "objectID": "course_guide.html#types-of-data-sampling-and-bias",
    "href": "course_guide.html#types-of-data-sampling-and-bias",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Types of Data, Sampling, and Bias",
    "text": "Types of Data, Sampling, and Bias\nFundamental to statistical analysis is understanding the types of data we encounter, the methods we use to collect them, and the potential sources of bias that can undermine the validity of our conclusions. We distinguish between categorical (qualitative) and quantitative (numerical) data. Categorical variables can further be broken down into nominal (color, baseball position, type of animal), ordinal (I had a very bad/somewhat bad/neutral/good/very good experience at CFT), and binary (Yes I passed Air Assault/No I did not). Quantitative data can also be broken down into either discrete or continuous. Understanding these distinctions is critical for selecting the correct tools for analysis and interpretation.\nThe two methods of sampling in this course will be through simple random sampling or convenience sampling. The difference in application is whether we can generalize our results to the larger population. As an example, say I do not have an exhaustive list of cadet ID numbers to randomly select from. Instead, I only can stand in Central Area after class at 1630 and survey the first 50 cadets that willingly take my survey. There is a certain sub-population I am probably missing (1st Reg, 4th Reg, Corps Squad, etc). This is a convenience sample. Instead, if I randomly select 50 C-Numbers from a complete list of the Corps obtained from the registrar, this would be a a true random sample of the Corps, whereas the former is what is known as selection bias.\nFinally, we explore the concept of bias in data collection. We identify common sources such as selection bias, response bias, and measurement bias, and discuss how poor sampling practices or flawed survey design can distort findings. This lesson sets the stage for the rest of the course by highlighting the importance of thoughtful data collection and critical evaluation of data sources.\nAs an example let us look at a dataset aggregated from over 50,000 diamonds:\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n1.02\nGood\nG\nVS2\n63.8\n59.0\n6080\n6.34\n6.27\n4.02\n\n\n0.31\nIdeal\nF\nVVS1\n61.9\n53.5\n882\n4.36\n4.39\n2.71\n\n\n0.60\nPremium\nD\nSI2\n61.3\n61.0\n1428\n5.46\n5.40\n3.33\n\n\n0.41\nIdeal\nE\nIF\n62.1\n54.0\n1419\n4.75\n4.81\n2.97\n\n\n0.72\nVery Good\nH\nVS1\n62.2\n54.0\n2877\n5.74\n5.76\n3.57\n\n\n1.20\nIdeal\nF\nVS2\n62.6\n56.0\n8486\n6.78\n6.73\n4.23\n\n\n\n\n\nEach of the rows is an individual diamond, generally called an observation. Each of the columns are unique aspects measured for every observation, called variables. Variables are either categorical, qualitative aspects of each measurement, or quantitative, a numbered entry."
  },
  {
    "objectID": "course_guide.html#exploratory-data-analysis",
    "href": "course_guide.html#exploratory-data-analysis",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nUnderstanding, communicating, and interpreting your data is paramount to any initial data analysis project. These are done through numerous visualizations and summary statistics which we will learn to regularly implement when given any new dataset.\n\nOne Variable – Visualizations and Summary Statistics\nStarting with a variable-by-variable approach is a natural first step. This is done rapidly in R with the following few functions:\n\n\n\n\n\n\n\n\n\nHistograms tell us where most of the values for a quantitative variable lie in its given distribution. We can determine skewness, a measure of how lopsided the data appear or if there are any asymmetries or tails.\n\n\n\n\n\n\n\n\n\nSimilar to a histogram, a boxplot will tell us exactly where the median, 1st and 3rd quartiles, and outliers exist for any quantitative variable. The ‘whiskers’ are determined by \\(1.5 \\times IQR\\) where the inter-quartile range is the \\(3rd - 1st\\) quartiles.\n\n\n\n\n\nVariable\nMean\nMedian\nSD\nVar\nMin\nMax\n\n\n\n\ncarat\n0.79\n0.70\n0.46\n0.21\n0.23\n3.24\n\n\ndepth\n61.77\n61.90\n1.40\n1.97\n56.30\n68.90\n\n\ntable\n57.48\n57.00\n2.26\n5.11\n50.00\n66.00\n\n\nprice\n3873.55\n2387.00\n3912.55\n15308046.06\n337.00\n18692.00\n\n\nx\n5.72\n5.68\n1.10\n1.21\n3.88\n9.44\n\n\ny\n5.72\n5.68\n1.09\n1.20\n3.90\n9.40\n\n\nz\n3.53\n3.52\n0.68\n0.46\n2.39\n5.85\n\n\n\n\n\nThe above are the predominant statistics you want to discern for every quantitative variable in your dataset. The benchmark location statistics are the mean, median, max, and min, while the standard deviation and variance are measures of how spread out the data are relative to one another.\n\\[\n\\begin{align}\n\\text{Sample Mean: } \\ \\ & \\bar{X} = \\frac{1}{n}\\sum_{i=1}^n x_i \\\\\n\\text{Sample Variance: } \\ \\ &  S^2 = \\frac{1}{n-1}\\sum_{i=1}^n ( x_i - \\bar{X}  )^2 \\\\\n\\text{Sample Standard Deviation:} \\ \\ & s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n ( x_i - \\bar{X} )^2}\n\\end{align}\n\\]\nNote: the standard deviation is the square root of the variance\n\n\nTwo Variables – Visualizations and Summary Statistics\n\n\n\n\n\n\n\n\n\nA scatterplot is the main tool to visualize and identify a relationship between two quantitative variables. Oftentimes, coloring each observation by another categorical variable is a way to maximize effectiveness of a single plot, as you are encoding more information within the same space.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ndepth\ntable\nprice\n\n\n\n\ncarat\n1.00\n0.00\n0.22\n0.91\n\n\ndepth\n0.00\n1.00\n-0.32\n-0.02\n\n\ntable\n0.22\n-0.32\n1.00\n0.17\n\n\nprice\n0.91\n-0.02\n0.17\n1.00\n\n\n\n\n\nCorrelation is the only multivariate summary statistic we will be using in this course, used to describe how two variables tend to move in tandem with one another. A perfect linear association evokes a correlation of 1, the opposite being a perfect negative association with a correlation of -1. No association is implied by a correlation near 0.\nMathematically: &gt; Definition &gt; For any two variables X,Y, the correlation of X and Y are: \\[\nr = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2\\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\]"
  },
  {
    "objectID": "course_guide.html#probability",
    "href": "course_guide.html#probability",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Probability",
    "text": "Probability\n\nSample Space and Random Experiment (\\(\\Omega\\))\nA random experiment is a process that produces an outcome which cannot be predicted with certainty in advance. It must be well-defined, have more than one possible outcome, and be repeatable under similar conditions. Each performance of the experiment results in a single outcome from the sample space. The sample space is the set of all possible outcomes of a random experiment.\n\n\nDefinition:\n\nThe sample space is the set of all possible outcomes of a random experiment, denoted \\(\\Omega\\).\nAn event is a subset of the sample space. It can represent one or more outcomes.\nIf all outcomes in \\(\\Omega\\) are equally likely, then for any event \\(A\\):\n\n\n\\[\n\\mathbb{P}(A) = \\frac{\\text{Number of outcomes in } A}{\\text{Total outcomes in } \\Omega}\n\\]\n\nExamples:\n\nTossing a coin once: \\(\\Omega = {\\text{Heads}, \\text{Tails}}\\)\nRolling a 6-sided die: \\(\\Omega = {1, 2, 3, 4, 5, 6}\\)\nLetter grade in MA206: \\(\\Omega = {A, B, C, D, F}\\)\nNumber of emails received in an hour: \\(\\Omega = {0, 1, 2, \\dots}\\)\n\n\n\n\nProbability Measure (\\(\\mathbb{P}\\))\nA probability measure is a rule, denoted \\(\\mathbb{P}\\), that assigns a number between 0 and 1 to every event in a collection of events (called a sigma-algebra, denoted \\(\\mathcal{F}\\) ). These probabilities must follow three key rules, known as the axioms of probability.\n\n\n\n\n\n\n\nAxioms of Probability\n\n\n\n\nNon-Negativity\nFor any event \\(A\\), the probability is never negative:\n\\[\n\\mathbb{P}(A) \\geq 0\n\\]\nNormalization\nThe probability of one of the events happening over the entire sample space is 1:\n\\[\n\\mathbb{P}(\\Omega) = 1\n\\]\nAdditivity (for disjoint events)\nIf events \\(A_1, A_2, A_3, \\dots\\) are mutually exclusive (no overlap), then the probability that any one of them occurs is the sum of their individual probabilities:\n\\[\n\\mathbb{P}\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\mathbb{P}(A_1) + \\mathbb{P}(A_2) + \\mathbb{P}(A_3) + \\cdots\n\\]\n\n\n\n\nExample (Simple):\nLet \\(A\\), \\(B\\), and \\(C\\) be outcomes when rolling a die:\n\n\\(A = \\{1\\}\\), $B = {3} $, \\(C = \\{5\\}\\)\n\nThese are disjoint events (they don’t overlap).\n\nThen: \\[\n\\mathbb{P}(A \\cup B \\cup C) = \\mathbb{P}(A) + \\mathbb{P}(B) + \\mathbb{P}(C) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{2}\n\\]\n\nThese three rules form the mathematical foundation of all probability calculations — everything else builds on them.\n\n\n\n\n\n\n\nSet Theory\n\n\n\n\nThe complement of an event \\(A\\), written \\(A^c\\), consists of all outcomes in \\(\\Omega\\) that are not in \\(A\\):\n\n\\[\n\\mathbb{P}(A) + \\mathbb{P}(A^c) = 1\n\\]\n\nThe intersection \\(A \\cap B\\) consists of outcomes where both \\(A\\) and \\(B\\) occur.\nThe union \\(A \\cup B\\) consists of outcomes where either \\(A\\), \\(B\\), or both occur:\n\n\\[\n\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\n\\]\n\nTwo events \\(A\\) and \\(B\\) are disjoint (mutually exclusive) if they cannot both occur:\n\n\\[\nA \\cap B = \\varnothing \\quad \\text{and} \\quad \\mathbb{P}(A \\cap B) = 0\n\\]\n\n\n\n\nConditional Probability\nFor events \\(A\\) and \\(B\\) with \\(0 &lt; P(B) \\le 1\\), the conditional probability of \\(A\\) given \\(B\\) is:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nExample: One card is drawn from a standard deck.\nLet \\(A\\): card is a Queen, and \\(B\\): card is a face card.\nFind \\(P(A)\\), \\(P(B)\\), and \\(P(A \\mid B)\\).\n\n\n\nLaw of Total Probability\n\n\n\n\n\n\nLoTP\n\n\n\nIf \\(E_1, \\dots, E_n\\) is a partition of the sample space (mutually exclusive and exhaustive), then for any event \\(A\\):\n\\[\nP(A) = \\sum_{i=1}^{n} P(E_i) P(A \\mid E_i)\n\\]\n\n\nExample:\nA fair die is rolled. Let event A: “an even number is rolled”.\nLet:\n- \\(E_1\\): roll is 1 or 2\n- \\(E_2\\): roll is 3 or 4\n- \\(E_3\\): roll is 5 or 6\n\nFind:\n- \\(P(A \\mid E_1)\\), \\(P(A \\mid E_2)\\), \\(P(A \\mid E_3)\\)\n- Then use the Law of Total Probability to find \\(P(A)\\)\n\n\n\nBayes’ Theorem\n\n\n\n\n\n\nSwitching the Conditioning of Events\n\n\n\n\nDefinition:\nIf \\(E_1, \\dots, E_n\\) is a partition of the sample space and \\(P(A) &gt; 0\\), then:\n\n\\[\nP(E_k \\mid A) = \\frac{P(A \\mid E_k) P(E_k)}{\\sum_{i=1}^n P(A \\mid E_i) P(E_i)}\n\\]\n\n\nExample:\nTwo urns:\n- Urn 1: 1 red, 1 blue\n- Urn 2: 3 red, 1 blue\nPick an urn at random, then draw one ball.\nIf the ball is red, what is the probability it came from Urn 1?\n\n\n\nCounting Principles\nBefore we can begin a thorough treatment of probability, some concepts in counting are needed to identify four common situations. These arise depending on when things are “allowed” to repeat or the “order” items are chosen in matters. The ability to discern when these four situations arise is more than half the battle.\n\nOrdered with Replacement\nThink of the number of ways of choosing a 4-digit passcode on your phone.\nThe order of the numbers matters, and you are allowed to repeat the same number. So how many arrangements are there? Since repetition is allowed and order matters, there are 10 digits for each position, giving:\n\\[\n\\text{Ordered Arrangements with Replacement} = n^r = 10^4 = 10{,}000\n\\]\n\n\nOrdered without Replacement\nThink of the number of ways I can create a batting order from 9 position players.\nThe order still matters, but players cannot be repeated. This is a permutation — an ordered arrangement without replacement.\n\\[\n{}_nP_r = P(n, r) = \\frac{n!}{(n - r)!}\n\\]\nNote: Factorial in math is computed as \\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\).\nFor example, the number of ways to assign the first 3 batting positions from 9 players:\n\\[\n{}_9P_3 = \\frac{9!}{(9-3)!} = 9 \\times 8 \\times 7 = 504\n\\]\n\n\nUnordered without Replacement\nThink of how many ways you can choose 3 scoops of ice cream from 5 unique flavors without repeats.\nBecause order doesn’t matter and repeats aren’t allowed, we use combinations:\n\\[\n{}_nC_k = \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\]\n\n\nUnordered with Replacement\nThink of how many different combinations of 3 scoop ice cream cones you can make with 5 unique flavors while allowing repeats.\nThis is the trickiest scenario. The order doesn’t matter, and repetition is allowed. The formula is:\n\\[\n\\text{Unordered Arrangements with Replacement} = \\binom{r+n-1}{r} = \\frac{(r+n-1)!}{r!(n-1)!}\n\\]\nExample: choosing 3 scoops from 5 flavors (with repeats):\n\\[\n\\binom{3+5-1}{3} = \\binom{7}{3} = 35\n\\]\nThis can be understood using the stars and bars method: selecting \\(r\\) scoops with \\(n-1\\) dividers. Imagine representing each scoop as a ★ (star) and using vertical bars | to separate flavor types. To choose \\(r\\) scoops from \\(n\\) flavors, you need \\(r\\) stars (for the scoops) and \\(n - 1\\) bars (to divide them into \\(n\\) categories). For example, if \\(r = 3\\) scoops and \\(n = 5\\) flavors, you arrange 3 stars and 4 bars in a row. One possible arrangement is ★ | ★★ | | |, which represents 1 scoop of flavor 1, 2 scoops of flavor 2, and 0 scoops of flavors 3, 4, and 5. The number of such arrangements is given by the combination formula \\(\\binom{r + n - 1}{r}\\), since you are choosing positions for the \\(r\\) indistinguishable stars among the \\(r + n - 1\\) total positions (stars and bars combined).\nNote: The above section may seem like it came out of nowhere, that is okay. A fundamental difficulty in probability is finding the sample space or event space due to finding the various different combinations/permutations sequences of different possibilities. To elaborate consider the next example: \n\nExample: No Matching Pairs in a Shoe Sample\nA closet contains \\(n\\) pairs of shoes (so \\(2n\\) total shoes). If \\(2r\\) shoes are chosen at random (where \\(2r &lt; n\\)), what is the probability that no matching pair is selected?\nWe are selecting \\(2r\\) shoes such that no left and right shoe from the same pair are both chosen.\nStrategy:\n1. First choose \\(2r\\) distinct pairs from the \\(n\\) available — there are \\(\\binom{n}{2r}\\) ways to do this.\n2. From each of these \\(2r\\) selected pairs, choose only one shoe (either left or right) — there are \\(2^{2r}\\) ways to do this.\n3. The total number of ways to choose any \\(2r\\) shoes out of \\(2n\\) is \\(\\binom{2n}{2r}\\).\nSo, the desired probability is:\n\\[\n\\mathbb{P}(\\text{No matching pair}) = \\frac{\\binom{n}{2r} \\cdot 2^{2r}}{\\binom{2n}{2r}}\n\\]"
  },
  {
    "objectID": "course_guide.html#random-variables-expectation-and-variance",
    "href": "course_guide.html#random-variables-expectation-and-variance",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Random Variables, Expectation, and Variance",
    "text": "Random Variables, Expectation, and Variance\n\n\n\n\n\n\nRandom Variable\n\n\n\nA random variable is a mapping that assigns a real number to every outcome in the sample space:\n\\[\nX: \\Omega \\rightarrow \\mathbb{R}\n\\]\n\n\n\n\n\n\n\n\nCumulative Distribution Function\n\n\n\nA cumulative distribution function (CDF) is a function \\(F_X: \\mathbb{R} \\rightarrow [0,1]\\) defined by:\n\\[\nF_X(x) = \\mathbb{P}(X \\le x)\n\\]"
  },
  {
    "objectID": "course_guide.html#discrete-random-variables",
    "href": "course_guide.html#discrete-random-variables",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\n\n\n\n\n\n\nDiscrete Random Variables\n\n\n\nA discrete random variable is a random variable that takes countably many values in \\(\\mathbb{R}\\).\nIts probability mass function is given by:\n\\[\nf_X(x) = \\mathbb{P}(X = x)\n\\]\nThe expected value (mean) of a discrete random variable \\(X\\) is:\n\\[\n\\mathbb{E}[X] = \\sum_x x \\cdot \\mathbb{P}(X = x)\n\\]\nDefinition (Variance):\nThe variance of a discrete random variable \\(X\\) is:\n\\[\n\\mathrm{Var}(X) = \\mathbb{E}\\left[(X - \\mathbb{E}[X])^2\\right] = \\sum_x (x - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}(X = x)\n\\]\n\n\n\nBinomial Distribution:\n\nLet \\(X \\sim \\text{Binomial}(n, p)\\) where \\(n \\in \\mathbb{N}\\) and \\(0 &lt; p &lt; 1\\).\n\nProbability Mass Function (PMF):\n\\[\n\\mathbb{P}(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k}, \\quad \\text{for } k = 0, 1, \\dots, n\n\\]\nCumulative Distribution Function (CDF):\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = \\sum_{k=0}^{\\lfloor x \\rfloor} \\binom{n}{k} p^k (1 - p)^{n - k}\n\\]\nExpected Value:\n\\[\n\\mathbb{E}[X] = np\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = np(1 - p)\n\\]\n\n\n\n\nGeometric Distribution:\n\nLet \\(X \\sim \\text{Geometric}(p)\\) be the number of trials until the first success (including the success), where \\(0 &lt; p &lt; 1\\).\n\nProbability Mass Function (PMF):\n\\[\n\\mathbb{P}(X = k) = (1 - p)^{k - 1} p, \\quad \\text{for } k = 1, 2, 3, \\dots\n\\]\nCumulative Distribution Function (CDF):\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = 1 - (1 - p)^{\\lfloor x \\rfloor}\n\\]\nExpected Value:\n\\[\n\\mathbb{E}[X] = \\frac{1}{p}\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = \\frac{1 - p}{p^2}\n\\]"
  },
  {
    "objectID": "course_guide.html#continuous-random-variables",
    "href": "course_guide.html#continuous-random-variables",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\n\n\n\n\n\nContinuous Random Variable\n\n\n\nA continuous random variable takes infinitely many values in \\(\\mathbb{R}\\). Its probability density function is given by:\n\\[\nf_X(x) = \\mathbb{P}(X = x)\n\\]\nNote: We do not find probabilities with the pdf like the pmf of a discrete random variable. We integrate over a neighborhood of the support of X, as there is no probability mass at any single point for a continuous distribution.\nThe expected value (mean) of a continuous random variable \\(X\\) is:\n\\[\n\\mathbb{E}[X] = \\int_{-\\infty}^\\infty x f_X(x)dx\n\\]\nDefinition (Variance):\nThe variance of a discrete random variable \\(X\\) is:\n\\[\n\\mathrm{Var}(X) = \\mathbb{E}\\left[(X - \\mathbb{E}[X])^2\\right] = \\int_{-\\infty}^\\infty (x - \\mathbb{E}[X])^2 f_X(x)dx = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\n\\]\n\n\n\nCentral Limit Theorem (CLT)\nThe Central Limit Theorem (CLT) is one of the most important results in statistics. It states that the sampling distribution of the sample mean \\(\\bar{X}\\) becomes approximately normal as the sample size \\(n\\) increases, regardless of the shape of the population distribution (provided it has finite mean and variance).\nSpecifically, if \\(X_1, X_2, \\dots, X_n\\) are i.i.d. random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then:\n\\[\n\\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\xrightarrow{d} \\mathcal{N}(0,1) \\quad \\text{as } n \\to \\infty\n\\]\nThis justifies the widespread use of the normal distribution to approximate sample means in practice.\n\n\n\n\nCredit: The New York Times\n\n\n\nNormal Distribution\nThe above video showed you the importance of this distribution, also called a Gaussian Distribution. Many natural phenomena are normally distributed.\n\nNormal Distribution\nLet \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), where \\(\\mu \\in \\mathbb{R}\\) and \\(\\sigma &gt; 0\\).\n\nProbability Density Function (PDF):\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right), \\quad x \\in \\mathbb{R}\n\\]\nCumulative Distribution Function (CDF):\nThere is no closed-form expression, but it is denoted as:\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = \\Phi\\left( \\frac{x - \\mu}{\\sigma} \\right)\n\\]\nwhere \\(\\Phi\\) is the standard normal CDF.\nExpected Value:\n\\[\n\\mathbb{E}[X] = \\mu\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = \\sigma^2\n\\]\n\n\n\n\nExponential Distribution\nThis distribution is helpful to model continuous time-related events: time between system failures at a factory, time between phone calls at a call center, time between insurance claims received at a insurance firm. All these can be modeled with this useful continuous random variable.\n\nLet \\(X \\sim \\text{Exponential}(\\lambda)\\) with \\(\\lambda &gt; 0\\).\n\nProbability Density Function (PDF):\n\\[\nf(x) = \\lambda e^{-\\lambda x}, \\quad x \\ge 0\n\\]\nCumulative Distribution Function (CDF):\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = 1 - e^{-\\lambda x}, \\quad x \\ge 0\n\\]\nExpected Value:\n\\[\n\\mathbb{E}[X] = \\frac{1}{\\lambda}\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = \\frac{1}{\\lambda^2}\n\\]"
  },
  {
    "objectID": "course_guide.html#confidence-intervals",
    "href": "course_guide.html#confidence-intervals",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nA confidence interval (CI) gives a range of plausible values for a population parameter based on a sample statistic. The general structure of any confidence interval is:\n\\[\n\\text{point estimate} \\ \\pm \\ \\text{margin of error}\n\\]\nMore specifically, for large samples or when the sampling distribution of the estimate is approximately normal:\n\\[\n\\text{CI} = \\hat{\\theta} \\ \\pm \\ z^* \\cdot \\text{SE}(\\hat{\\theta})\n\\]\nWhere:\n\n\\(\\hat{\\theta}\\) is the point estimate (e.g., \\(\\bar{x}\\) for the mean, \\(\\hat{p}\\) for a proportion)\n\\(z^*\\) is the critical value from the standard normal distribution (e.g., 1.96 for 95% confidence)\n\\(\\text{SE}(\\hat{\\theta})\\) is the standard error of the estimate\n\nThis structure applies to many common settings:\n\nCI for a population mean: \\[\n\\bar{x} \\pm z^* \\cdot \\frac{s}{\\sqrt{n}}\n\\]\nCI for a population proportion: \\[\n\\hat{p} \\pm z^* \\cdot \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\n\\]\n\n\nInterpretation:\n\n“We are 95% confident that the true population parameter lies within this interval.”\n\nThis does not mean there’s a 95% probability the parameter is in the interval — rather, it means that 95% of all intervals computed from repeated samples in this manner would contain the true parameter."
  },
  {
    "objectID": "course_guide.html#one-sample-hypothesis-testing",
    "href": "course_guide.html#one-sample-hypothesis-testing",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "One Sample Hypothesis Testing",
    "text": "One Sample Hypothesis Testing\nHypothesis testing is a formal method for making inferences about a population using sample data. The whole test aspect is questioning if the statistic your sample shows is significantly different than a certain value in question. You have two underlying premises, referred to as the null and alternative hypotheses. The null hypothesis assumes that there is no difference: the statistic from your value is the same as the value you are testing. The alternative conflicts the null and says they are different. The process involves:\n\nState the null and alternative hypotheses. There are three different variants to create your hypotheses statements depending on what the question being asked entails.\n\n\nGreater than Alternative Hypothesis\n\n\\[\n\\begin{align}\nH_0: Parameter &= Value \\ in \\ Question \\\\\nH_A: Parameter &&gt; Value \\ in \\ Question\n\\end{align}\n\\]\nThe entire inference aspect of hypothesis testing is that you are using your sample statistic, a tangible aspect of your data, to make an argument about the population parameter, an entity that is unknown to you. This is why the hypotheses are written in terms of the parameter. You are testing whether an aspect or parameter about the population is greater than a benchmark value decided by you in advance.\n\nLess than Alternative Hypothesis\n\n\\[\n\\begin{align}\nH_0: Parameter &= Value \\ in \\ Question \\\\\nH_A: Parameter &&lt; Value \\ in \\ Question\n\\end{align}\n\\]\nVery similarly, the less than hypothesis is also a one-sided hypothesis test in that you are only testing one side of the value, abeit this time if the population parameter is less than the tested value.\n\nNot equal to Alternative Hypothesis\n\n\\[\n\\begin{align}\nH_0: Parameter &= Value \\ in \\ Question \\\\\nH_A: Parameter &\\neq Value \\ in \\ Question\n\\end{align}\n\\]\nThis is the only two-sided hypothesis test, denoted with the not equal to alternative hypothesis. Both sides must be accounted for in this test, and therefore as we will see shortly require more evidence for significance.\n\nChoose a significance level \\(\\alpha\\).\n\nThis is the threshold you will also decide to inform your certainty in your conclusions. As seen previously, unless you are finding the probability that something will happen in the entire sample space (which happens probability 1); ie, there are no absolutes. Therefore there is always a chance that your conclusion will be wrong. Here is where you decide how “often” you are willing to be wrong. Is it 1% of the time? 5% of the time? Think of the significance level as choosing the percentage of the time you are willing to be wrong in your conclusion, (I know this sounds weird). A common \\(\\alpha\\) is 5%.\n\nCompute the test statistic.\n\nCompute the relevant summary statistic. In this course you will either be calculating a sample proportion, denoted \\(\\hat{p}\\) if the variable of interest is categorical or the sample mean \\(\\bar{X}\\) if quantitative.\n\nStandardize the test statistic.\n\nThis step transforms your statistic so it can be treated as a random variable from a named distribution. For proportions this will be a Normal Random Variable, however if quantitative it will be a t-distributed random variable.\n\n\n\n\nCredit: 365 Data Science\n\nDetermine the p-value.\n\nOnce we have the standardized statistic, it can be treated as either a Standard Normal Random Variable or Student’s-t Random Variable. This is where the alternative hypotheses come in to play to determine the p-value: the probability that if the null hypothesis is indeed true you choose to make an argument supporting the alternative. To find the probability of a certain event happening in a continuous random variable, you integrate the probability density function with the limits of integration being the range of values the random variable could take. Both the Standard Normal and Student’s-t are continuous, so depending on your alternative hypothesis, your p-value is calculated by either of the following:\n\\[\n\\begin{align}\n\\text{Greater Hypothesis} &&&& \\text{Less than Hypothesis} &&&& \\text{Two Sided Hypothesis} \\\\\n\\mathbb{P}(X&gt;z) = \\int_z^\\infty f_X(x) dx &&&& \\mathbb{P}(X&lt;z) = \\int_{-\\infty}^z f_X(x)dx &&&& \\mathbb{P}(X&gt;z) = \\int_{|z|}^\\infty f_X(x)dx + \\int_{-\\infty}^{-|z|} f_X(x)dx\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\n\nMaking a conclusion based on comparison.\n\nOnce a p-value is obtained, reference it to the significance level chosen. If the p-value is greater than \\(\\alpha\\), you fail to reject the null hypothesis, if it is smaller, you reject the null hypothesis, and then state what that means in the context of the problem.\n\n\nSingle Proportion\nWe conduct inference on a population proportion \\(\\pi\\) relative to a hypothesized value \\(\\pi_0\\). The test statistic is:\n\\[\nz = \\frac{\\hat{p} - \\pi}{\\sqrt{\\frac{\\pi(1 - \\pi)}{n}}}\n\\]\n\nExample: Test if more than 40% of diamonds are “Ideal” cut\n\n# Null hypothesis: p = 0.20\npi &lt;- 0.4\nn &lt;- nrow(diamonds)\nphat &lt;- mean(diamonds$cut == \"Ideal\")\n\n# Test statistic and p-value\nz &lt;- (phat - pi) / sqrt(pi * (1 - pi) / n)\np_value &lt;- 1 - pnorm(z)\n\ncat(\"Z-statistic:\", round(z, 3), \"\\n\")\n\nZ-statistic: -0.22 \n\ncat(\"P-value:\", round(p_value, 4))\n\nP-value: 0.587\n\n\n\n\n\n\nSingle Mean\nWe conduct inference on a population mean \\(\\mu\\) relative to a hypothesized value \\(\\mu_0\\). The test statistic is:\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\nExample: Test if the average diamond price is $4000.\n\nmu0 &lt;- 4000\nx_bar &lt;- mean(diamonds$price)\ns &lt;- sd(diamonds$price)\nn &lt;- length(diamonds$price)\n\n# Test statistic and p-value\nt_stat &lt;- (x_bar - mu0) / (s / sqrt(n))\np_value &lt;- 2*(1 - pt(abs(t_stat), df = n - 1))\n\ncat(\"T-statistic:\", round(t_stat, 3), \"\\n\")\n\nT-statistic: -3.912 \n\ncat(\"P-value:\", round(p_value, 4))\n\nP-value: 1e-04\n\n\n\n\n\nExperimental Design: Power, Type I / Type II Error\nIn any hypothesis test, we face the possibility of making incorrect conclusions. These are formalized through Type I and Type II errors:\n\nType I Error (\\(\\alpha\\)): Rejecting the null hypothesis when it is actually true. This is controlled by the significance level of the test, often set to \\(\\alpha = 0.05\\).\nType II Error (\\(\\beta\\)): Failing to reject the null hypothesis when the alternative is actually true. This is harder to control and depends on the true parameter, sample size, and variance.\nPower of the Test: The probability of correctly rejecting the null hypothesis when the alternative is true: \\[\n\\text{Power} = 1 - \\beta\n\\]\n\nA powerful test detects meaningful effects and minimizes Type II error. Power increases when: - Sample size increases (\\(n \\uparrow\\)) - Effect size increases (true parameter is farther from null) - Variability decreases (standard deviation \\(\\downarrow\\)) - Significance level \\(\\alpha\\) increases (easier to reject null)\n\n\n\n\n\n\n\n\n\n\n\nThis diagram shows: - The blue curve is the null distribution (centered at 0). - The red dashed curve is the alternative distribution (shifted mean). - The dotted vertical line is the critical value (e.g., \\(z = 1.645\\) for \\(\\alpha = 0.05\\) in a one-sided test). - Area to the right of this cutoff under the null curve is \\(\\alpha\\). - Area to the left of this cutoff under the alternative curve is \\(\\beta\\). - The remaining area under the red curve (right tail) is power."
  },
  {
    "objectID": "course_guide.html#two-sample-hypothesis-testing",
    "href": "course_guide.html#two-sample-hypothesis-testing",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Two Sample Hypothesis Testing",
    "text": "Two Sample Hypothesis Testing\n\nDifference of Proportions\nWe compare two population proportions to determine whether there is a significant difference between them. The test statistic is:\n\\[\nz = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1 - \\hat{p}) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}}\n\\]\nWhere \\(\\hat{p}\\) is the pooled proportion.\n\nExample: Are “Ideal” cuts more common among Premium vs. Good color grades?\n\n# Subset to just Premium and Good clarity levels\ndf &lt;- diamonds %&gt;% filter(color ==\"D\" | color==\"E\")\n\ntable(df$cut, df$color)\n\n           \n               D    E    F    G    H    I    J\n  Fair       163  224    0    0    0    0    0\n  Good       662  933    0    0    0    0    0\n  Very Good 1513 2400    0    0    0    0    0\n  Premium   1603 2337    0    0    0    0    0\n  Ideal     2834 3903    0    0    0    0    0\n\n# Create a binary outcome: Ideal or not\ndf &lt;- df %&gt;%\n  mutate(is_ideal = cut == \"Ideal\")\n\n# Proportions\np1 &lt;- mean(df$is_ideal[df$color == \"D\"])\np2 &lt;- mean(df$is_ideal[df$color == \"E\"])\nn1 &lt;- sum(df$color == \"D\")\nn2 &lt;- sum(df$color == \"E\")\n\n# Pooled proportion\nphat &lt;- (p1 * n1 + p2 * n2) / (n1 + n2)\n\n# Test statistic\nz &lt;- (p1 - p2) / sqrt(phat * (1 - phat) * (1 / n1 + 1 / n2))\np_value &lt;- 2 * (1 - pnorm(abs(z)))\n\ncat(\"Z-statistic:\", round(z, 3), \"\\n\")\n\nZ-statistic: 2.566 \n\ncat(\"P-value:\", round(p_value, 4))\n\nP-value: 0.0103\n\n\n\n\n\n\nMultiple Proportions (Chi-Square Test of Independence)\nUsed when comparing proportions across more than two groups.\n\nExample: Is cut independent of color?\n\ntbl &lt;- table(diamonds$cut, diamonds$color)\nchisq.test(tbl)\n\n\n    Pearson's Chi-squared test\n\ndata:  tbl\nX-squared = 310.32, df = 24, p-value &lt; 2.2e-16\n\n\nThis test checks whether the distribution of cut types is independent of the diamond color. A small p-value suggests a dependency.\n\n\n\n\nDifference of Means\nUsed to compare two independent sample means.\n\nExample: Is the average price different between “Ideal” and “Fair” cuts?\n\ndf &lt;- diamonds %&gt;% filter(cut %in% c(\"Ideal\", \"Fair\"))\n\nt.test(price ~ cut, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  price by cut\nt = 9.7484, df = 1894.8, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Fair and group Ideal is not equal to 0\n95 percent confidence interval:\n  719.9065 1082.5251\nsample estimates:\n mean in group Fair mean in group Ideal \n           4358.758            3457.542 \n\n\nThis performs a two-sample t-test, assuming unequal variances by default. The null hypothesis is that the means are equal.\n\n\n\n\nPaired Data\nIn paired designs, each observation in one group is paired with a related observation in the other. Since diamonds has no natural pairing, we’ll simulate a paired example.\n\nExample (Simulated): Price before and after resizing a set of diamonds\n\nset.seed(123)\n\n# Simulate paired prices: original and discounted\nn &lt;- 100\noriginal_price &lt;- sample(diamonds$price, n)\ndiscounted_price &lt;- original_price * runif(n, 0.85, 0.95)\n\nt.test(original_price, discounted_price, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  original_price and discounted_price\nt = 8.9496, df = 99, p-value = 2.132e-14\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 342.2012 537.1645\nsample estimates:\nmean difference \n       439.6829 \n\n\nThis tests whether the mean price before and after a simulated discount differs significantly."
  },
  {
    "objectID": "course_guide.html#regression",
    "href": "course_guide.html#regression",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Regression",
    "text": "Regression\nIn this section we continue our multivariate inference with creating models for the purpose of identifying significance between explanatory (predictor) variables and the response. The function used to create the model, \\(\\hat{y_i}=f(x_i)\\) will make predictions, known as fitted values. Do to the variability in the data, these fitted values will not exactly predict the response (ie. \\(y_i \\neq \\hat{y}\\)) for all values in the response. These errors are the deviations from the response and the fitted values and are referred as residuals, with notation \\(\\epsilon_i = y_i - \\hat{y}_i\\).\nTo assess how well a model performs, the residuals are summarized in a few different methods:\n\nMean Absolute Deviation\nThe average magnitude of the residuals:\n\\[\nMAD = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|\n\\]\n\n\nMean Squared Error:\nThe average magnitude of the residual-squared.\n\\[\nMSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\n\n\nSimple Linear Regression\nThe above metrics could be applied to any model, however the central method to assess a linear relationship between two quantitative variables isSimple Linear Regression, or better known as the line of best fit:\n\\[\n\\hat{y} = \\beta_0 + \\beta_1 x\n\\]\nThe \\(\\beta\\)’s are the parameters of the model: the y-intercept and slope. The reason the method is the best fit is because we optimizes the choices for these two parameters by minimizing the sum of squared error:\n\\[\n\\begin{align}\nSSE &= \\sum_{i=1}^n (\\epsilon_i)^2 \\\\\n    &= \\sum_{i=1}^n (y_i - \\widehat{y}_i)^2 \\\\\n    &= \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 \\\\\n\\frac{\\partial}{\\partial \\beta_0}SSE &= \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i)^2 \\\\\n0 &= -2\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i) \\\\\n\\widehat{\\beta}_0 &= \\bar{y} - \\widehat{\\beta}_1 \\bar{x} \\\\\n\\frac{\\partial}{\\partial \\beta_1}SSE &= \\frac{\\partial}{\\partial \\beta_1} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i)^2 \\\\\n&= \\frac{\\partial}{\\partial \\beta_1} \\sum_{i=1}^n (y_i - (\\bar{y} - \\widehat{\\beta}_1 \\bar{x}) - \\widehat{\\beta}_1x_i)^2 \\\\\n0 &= -2\\sum_{i=1}^n (\\bar{x} - x_i)(y_i - \\bar{y} + \\widehat{\\beta}_1( \\bar{x} - x_i)) \\\\\n\\widehat{\\beta}_1 \\sum_{i=1}^n (x_i - \\bar{x})^2 &= \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\\\\n\\widehat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)}\n\\end{align}\n\\]\nThe above is beyond the scope of this course, however it warrants a healthy appreciation for finding the line of best fit!\nIn practice, from the diamonds dataset we could model price as a function of carat:\n\n# Sample and fit model\nset.seed(206)\ndf &lt;- ggplot2::diamonds %&gt;% sample_n(1000)\nlm_simple &lt;- lm(price ~ carat, data = df)\nsummary(lm_simple)\n\n\nCall:\nlm(formula = price ~ carat, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8077.3  -813.1    10.2   607.5  8759.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2376.64      93.40  -25.45   &lt;2e-16 ***\ncarat        7885.65      99.37   79.35   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1499 on 998 degrees of freedom\nMultiple R-squared:  0.8632,    Adjusted R-squared:  0.8631 \nF-statistic:  6297 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nThere is a lot in the summary output, however the main ideas here lie in the magnitude and sign of the coefficient, looking for practical significance, and also looking at the size of the p-value relative to a chosen \\(\\alpha\\), checking for statistical significance.\nYou may be wondering in a line of best fit, where did the p-value come from? Good question! In addition to finding the line of best fit, our linear model assesses the relevance of all parameters in the model. This assessment is a *one-sample t-test for every ! If there is significance, then there is a significant assocation between the explanatory variable and the response.\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Linear Regression\nWe can extend linear regression to in fact include as many predictor variables as we want (as long as we have more observations than variables!). This is implemented through Multiple Linear Regression:\n\\[\n\\widehat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n\\]\nThe derivation would be too lengthy to do them individually previously, however through Matrix Algebra (MA371 anyone?!), the solved vector of coefficients, \\(\\widehat{\\beta}_{p\\times1}\\) has the following solution:\n\\[\n\\widehat\\beta_{p\\times 1} = (X_{p\\times n}^TX_{n \\times p})^{-1}X_{p \\times n}^T \\vec{y}_{n \\times 1}\n\\] Observe the subscripts for the dimensions of the matrices, ending with a \\(p \\times 1\\) vector for the coefficients that minimize the SSE.\nHere, we use carat, depth, and table to predict price.\n\nlm_multi &lt;- lm(price ~ carat + depth + table, data = df)\nsummary(lm_multi)\n\n\nCall:\nlm(formula = price ~ carat + depth + table, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8252.9  -784.5   -22.4   624.7  8211.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13392.24    2845.47   4.707 2.88e-06 ***\ncarat        7965.09      99.01  80.445  &lt; 2e-16 ***\ndepth        -146.58      35.32  -4.149 3.62e-05 ***\ntable        -118.01      22.05  -5.351 1.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1475 on 996 degrees of freedom\nMultiple R-squared:  0.8679,    Adjusted R-squared:  0.8675 \nF-statistic:  2181 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nYou can compare \\(R^2\\) values and p-values to determine whether the additional variables meaningfully improve the model.\n\n\n\nGoodness of Fit\nWe use several metrics to assess the quality of a regression model:\n\n\\(R^2\\): Proportion of variance in the response explained by the predictors.\nResidual Standard Error (RSE): Average size of the residuals.\nF-statistic: Overall significance of the regression.\nResidual Plots: Visual diagnostics to assess assumptions.\n\n\n# Residual plot\nplot(lm_multi, which = 1)  # Residuals vs Fitted\n\n\n\n\n\n\n\n\n\n# Histogram of residuals\nresiduals &lt;- resid(lm_multi)\nhist(residuals, breaks = 30, col = \"lightblue\", main = \"Histogram of Residuals\", xlab = \"Residual\")\n\n\n\n\n\n\n\n\nWe want residuals to be roughly normally distributed and randomly scattered around zero to satisfy assumptions of linear regression.\n\n\nANOVA\nAnalysis of Variance (ANOVA) is a statistical method used to compare the means of three or more groups to determine if at least one of the group means is significantly different from the others.\n\n\nTheoretical Foundation\nANOVA works by partitioning the total variability in the data into two components: - Between-group variability: how much the group means differ from the overall mean. - Within-group variability: how much individual observations vary within each group.\nThe core idea is that if the between-group variability is large relative to the within-group variability, then at least one group mean is likely different.\n[ F = = ]\nWhere: - ( SSB ) = Sum of Squares Between - ( SSW ) = Sum of Squares Within - ( k ) = number of groups - ( n ) = total number of observations\nIf the calculated F-statistic is large, and the p-value is small (typically &lt; 0.05), we reject the null hypothesis:\n\n( H_0: _1 = _2 = = _k ) (all group means are equal)\n( H_A: ) At least one group mean is different\n\n\n\n\nExample: Do Different Diamond Cuts Have Different Average Prices?\nWe’ll use the diamonds dataset and compare the mean price across the five levels of the cut variable.\n\n# Sample for speed\nset.seed(206)\ndf &lt;- diamonds %&gt;% sample_n(1000)\n\n# Summary statistics by cut\ndf %&gt;%\n  group_by(cut) %&gt;%\n  summarise(mean_price = mean(price), n = n())\n\n# A tibble: 5 × 3\n  cut       mean_price     n\n  &lt;ord&gt;          &lt;dbl&gt; &lt;int&gt;\n1 Fair           3677.    28\n2 Good           3429.    74\n3 Very Good      4561.   226\n4 Premium        4355.   273\n5 Ideal          3590.   399\n\n\n\n\n\nRun ANOVA Test\n\n# One-way ANOVA: price ~ cut\nanova_model &lt;- aov(price ~ cut, data = df)\nsummary(anova_model)\n\n             Df    Sum Sq  Mean Sq F value Pr(&gt;F)  \ncut           4 1.996e+08 49901236   3.065  0.016 *\nResiduals   995 1.620e+10 16283224                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis will output the F-statistic and p-value. A small p-value (e.g. &lt; 0.05) suggests that at least one cut has a significantly different mean price.\n\n\n\n\nVisualize the Group Differences\n\nggplot(df, aes(x = cut, y = price)) +\n  geom_boxplot(fill = \"lightblue\") +\n  labs(title = \"Boxplot of Diamond Price by Cut\",\n       x = \"Cut\", y = \"Price ($)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nBoxplots help visualize both the median and spread of price within each cut level.\n\n\n\nFollow-Up: Which Cuts Are Different?\nIf the ANOVA result is significant, we can follow up with a Tukey HSD test to identify which group pairs differ.\n\nTukeyHSD(anova_model)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = price ~ cut, data = df)\n\n$cut\n                        diff        lwr        upr     p adj\nGood-Fair         -247.90830 -2694.6097 2198.79307 0.9987125\nVery Good-Fair     884.55594 -1324.7679 3093.87977 0.8096253\nPremium-Fair       678.19322 -1510.0655 2866.45192 0.9157755\nIdeal-Fair         -86.59085 -2242.4691 2069.28738 0.9999672\nVery Good-Good    1132.46424  -344.4875 2609.41597 0.2227928\nPremium-Good       926.10152  -519.1496 2371.35262 0.4030816\nIdeal-Good         161.31745 -1234.4209 1557.05582 0.9978434\nPremium-Very Good -206.36272 -1198.0859  785.36051 0.9795621\nIdeal-Very Good   -971.14679 -1889.2153  -53.07827 0.0320166\nIdeal-Premium     -764.78408 -1630.9331  101.36495 0.1126355\n\n\nThis test controls the family-wise error rate and gives pairwise confidence intervals and p-values.\n\n\n\nInterpretation\n\nIf p &lt; 0.05 in the ANOVA, we conclude that at least one cut differs in mean price.\nUse TukeyHSD to find out which cuts are significantly different.\nANOVA assumes:\n\nIndependent observations\nNormally distributed residuals\nEqual variances across groups (can check with Levene’s test)"
  },
  {
    "objectID": "code_annex.html",
    "href": "code_annex.html",
    "title": "MA206: Code Annex",
    "section": "",
    "text": "Adding Dependencies\nIn RStudio we use a common framework called tidyverse which holds a plethora of useful functions to work with and manipulate data. The other packages are not always necessary, only add the libraries you need at the beginning of your code document. If you do not have that library installed, in the console tab in the bottom-left pane of your RStudio use the function install.packages('the_package_you_want_to_install') and RStudio will install it for you.\n\n\nData Sampling\n\nset.seed(1991)\ndata(diamonds)\ndf &lt;- diamonds %&gt;% sample_n(size = 1000)\n\n\n\nTable Preview\n\ndf %&gt;% head() %&gt;%\n  kable(\"html\", align = \"c\") %&gt;%\n  kable_styling(\n    full_width = FALSE,\n    position = \"center\",\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"bordered\")\n  )\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n1.02\nGood\nG\nVS2\n63.8\n59.0\n6080\n6.34\n6.27\n4.02\n\n\n0.31\nIdeal\nF\nVVS1\n61.9\n53.5\n882\n4.36\n4.39\n2.71\n\n\n0.60\nPremium\nD\nSI2\n61.3\n61.0\n1428\n5.46\n5.40\n3.33\n\n\n0.41\nIdeal\nE\nIF\n62.1\n54.0\n1419\n4.75\n4.81\n2.97\n\n\n0.72\nVery Good\nH\nVS1\n62.2\n54.0\n2877\n5.74\n5.76\n3.57\n\n\n1.20\nIdeal\nF\nVS2\n62.6\n56.0\n8486\n6.78\n6.73\n4.23\n\n\n\n\n\n\n\nHistograms\n\np1 &lt;- df %&gt;% ggplot(aes(x = carat)) + geom_histogram(bins = 30) + theme_minimal()\np2 &lt;- df %&gt;% ggplot(aes(x = depth)) + geom_histogram(bins = 30) + theme_minimal()\np3 &lt;- df %&gt;% ggplot(aes(x = price)) + geom_histogram(bins = 30) + theme_minimal()\np1 | p2 | p3\n\n\n\n\n\n\n\n\n\n\nBoxplots\n\np4 &lt;- df %&gt;% ggplot(aes(x = carat)) + geom_boxplot() + theme_minimal()\np5 &lt;- df %&gt;% ggplot(aes(x = depth)) + geom_boxplot() + theme_minimal()\np6 &lt;- df %&gt;% ggplot(aes(x = price)) + geom_boxplot() + theme_minimal()\np4 | p5 | p6\n\n\n\n\n\n\n\n\n\n\nSummary Statistics Table\n\ndf %&gt;%\n  summarise(\n    across(\n      where(is.numeric),\n      list(\n        Mean = ~mean(.x, na.rm = TRUE),\n        Median = ~median(.x, na.rm = TRUE),\n        SD = ~sd(.x, na.rm = TRUE),\n        Var = ~var(.x, na.rm = TRUE),\n        Min = ~min(.x, na.rm = TRUE),\n        Max = ~max(.x, na.rm = TRUE)\n      ),\n      .names = \"{.col}_{.fn}\"\n    )\n  ) %&gt;%\n  round(2) %&gt;%\n  pivot_longer(everything(), names_to = c(\"Variable\", \"Stat\"), names_sep = \"_\") %&gt;%\n  pivot_wider(names_from = Stat, values_from = value) %&gt;%\n  kable(\"html\", align = \"c\") %&gt;%\n  kable_styling(\n    full_width = FALSE,\n    position = \"center\",\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"bordered\")\n  )\n\n\n\n\nVariable\nMean\nMedian\nSD\nVar\nMin\nMax\n\n\n\n\ncarat\n0.79\n0.70\n0.46\n0.21\n0.23\n3.24\n\n\ndepth\n61.77\n61.90\n1.40\n1.97\n56.30\n68.90\n\n\ntable\n57.48\n57.00\n2.26\n5.11\n50.00\n66.00\n\n\nprice\n3873.55\n2387.00\n3912.55\n15308046.06\n337.00\n18692.00\n\n\nx\n5.72\n5.68\n1.10\n1.21\n3.88\n9.44\n\n\ny\n5.72\n5.68\n1.09\n1.20\n3.90\n9.40\n\n\nz\n3.53\n3.52\n0.68\n0.46\n2.39\n5.85\n\n\n\n\n\n\n\nScatterplots\n\np7 &lt;- df %&gt;% ggplot(aes(x = carat, y = price, color = clarity)) + geom_point() + theme_minimal()\np8 &lt;- df %&gt;% ggplot(aes(x = table, y = price, color = cut)) + geom_point() + theme_minimal()\np9 &lt;- df %&gt;% ggplot(aes(x = depth, y = price, color = color)) + geom_point() + theme_minimal()\np7 | p8 | p9\n\n\n\n\n\n\n\n\n\n\nBar Charts\n\np10 &lt;- df %&gt;% group_by(cut) %&gt;% summarise(avg_price = mean(price)) %&gt;%\n  ggplot(aes(x = cut, y = avg_price)) +\n  geom_bar(stat = \"identity\", fill = \"pink\") +\n  labs(title = \"Average Price by Diamond Cut\", x = \"Cut\", y = \"Avg Price\") +\n  theme_minimal()\n\np11 &lt;- df %&gt;% group_by(color) %&gt;% summarise(avg_price = mean(price)) %&gt;%\n  ggplot(aes(x = color, y = avg_price)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Average Price by Diamond Color\", x = \"Color\", y = \"Avg Price\") +\n  theme_minimal()\n\np12 &lt;- df %&gt;% group_by(clarity) %&gt;% summarise(avg_price = mean(price)) %&gt;%\n  ggplot(aes(x = clarity, y = avg_price)) +\n  geom_bar(stat = \"identity\", fill = \"magenta\") +\n  labs(title = \"Average Price by Diamond Clarity\", x = \"Clarity\", y = \"Avg Price\") +\n  theme_minimal()\n\np10 | p11 | p12\n\n\n\n\n\n\n\n\n\n\nCorrelation Table\n\ndf %&gt;%\n  select(where(is.numeric)) %&gt;%\n  cor(use = \"pairwise.complete.obs\") %&gt;%\n  round(2) %&gt;%\n  kable(\"html\", align = \"c\") %&gt;%\n  kable_styling(\n    full_width = FALSE,\n    position = \"center\",\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"bordered\")\n  )\n\n\n\n\n\ncarat\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\ncarat\n1.00\n0.00\n0.22\n0.91\n0.98\n0.98\n0.98\n\n\ndepth\n0.00\n1.00\n-0.32\n-0.02\n-0.06\n-0.07\n0.06\n\n\ntable\n0.22\n-0.32\n1.00\n0.17\n0.24\n0.24\n0.20\n\n\nprice\n0.91\n-0.02\n0.17\n1.00\n0.88\n0.88\n0.88\n\n\nx\n0.98\n-0.06\n0.24\n0.88\n1.00\n1.00\n0.99\n\n\ny\n0.98\n-0.07\n0.24\n0.88\n1.00\n1.00\n0.99\n\n\nz\n0.98\n0.06\n0.20\n0.88\n0.99\n0.99\n1.00\n\n\n\n\n\n\n\nProportion Test Example\n\npi &lt;- 0.4\nn &lt;- nrow(diamonds)\nphat &lt;- mean(diamonds$cut == \"Ideal\")\nz &lt;- (phat - pi) / sqrt(pi * (1 - pi) / n)\np_value &lt;- 1 - pnorm(z)\ncat(\"Z-statistic: \", round(z, 3), \"\\nP-value: \", round(p_value, 4))\n\nZ-statistic:  -0.22 \nP-value:  0.587\n\n\n\n\nMean Test Example\n\nmu0 &lt;- 4000\nx_bar &lt;- mean(diamonds$price)\ns &lt;- sd(diamonds$price)\nn &lt;- length(diamonds$price)\nt_stat &lt;- (x_bar - mu0) / (s / sqrt(n))\np_value &lt;- 2 * (1 - pt(abs(t_stat), df = n - 1))\ncat(\"T-statistic: \", round(t_stat, 3), \"\\nP-value: \", round(p_value, 4))\n\nT-statistic:  -3.912 \nP-value:  1e-04\n\n\n\n\nTwo-Sample Proportion Test\n\ndf_prop &lt;- diamonds %&gt;% filter(color %in% c(\"D\", \"E\")) %&gt;% mutate(is_ideal = cut == \"Ideal\")\np1 &lt;- mean(df_prop$is_ideal[df_prop$color == \"D\"])\np2 &lt;- mean(df_prop$is_ideal[df_prop$color == \"E\"])\nn1 &lt;- sum(df_prop$color == \"D\")\nn2 &lt;- sum(df_prop$color == \"E\")\nphat_pool &lt;- (p1 * n1 + p2 * n2) / (n1 + n2)\nz &lt;- (p1 - p2) / sqrt(phat_pool * (1 - phat_pool) * (1 / n1 + 1 / n2))\np_value &lt;- 2 * (1 - pnorm(abs(z)))\ncat(\"Z-statistic: \", round(z, 3), \"\\nP-value: \", round(p_value, 4))\n\nZ-statistic:  2.566 \nP-value:  0.0103\n\n\n\n\nChi-Square Test\n\ntbl &lt;- table(diamonds$cut, diamonds$color)\nchisq.test(tbl)\n\n\n    Pearson's Chi-squared test\n\ndata:  tbl\nX-squared = 310.32, df = 24, p-value &lt; 2.2e-16\n\n\n\n\nIndependent T-Test\n\ndf_t &lt;- diamonds %&gt;% filter(cut %in% c(\"Ideal\", \"Fair\"))\nt.test(price ~ cut, data = df_t)\n\n\n    Welch Two Sample t-test\n\ndata:  price by cut\nt = 9.7484, df = 1894.8, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Fair and group Ideal is not equal to 0\n95 percent confidence interval:\n  719.9065 1082.5251\nsample estimates:\n mean in group Fair mean in group Ideal \n           4358.758            3457.542 \n\n\n\n\nPaired T-Test (Simulated)\n\nset.seed(123)\nn &lt;- 100\norig &lt;- sample(diamonds$price, n)\ndisc &lt;- orig * runif(n, 0.85, 0.95)\nt.test(orig, disc, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  orig and disc\nt = 8.9496, df = 99, p-value = 2.132e-14\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 342.2012 537.1645\nsample estimates:\nmean difference \n       439.6829 \n\n\n\n\nSimple Linear Regression\n\nset.seed(206)\ndf_lm &lt;- diamonds %&gt;% sample_n(1000)\nlm_simple &lt;- lm(price ~ carat, data = df_lm)\nsummary(lm_simple)\n\n\nCall:\nlm(formula = price ~ carat, data = df_lm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8077.3  -813.1    10.2   607.5  8759.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2376.64      93.40  -25.45   &lt;2e-16 ***\ncarat        7885.65      99.37   79.35   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1499 on 998 degrees of freedom\nMultiple R-squared:  0.8632,    Adjusted R-squared:  0.8631 \nF-statistic:  6297 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nMultiple Linear Regression\n\nlm_multi &lt;- lm(price ~ carat + depth + table, data = df_lm)\nsummary(lm_multi)\n\n\nCall:\nlm(formula = price ~ carat + depth + table, data = df_lm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8252.9  -784.5   -22.4   624.7  8211.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13392.24    2845.47   4.707 2.88e-06 ***\ncarat        7965.09      99.01  80.445  &lt; 2e-16 ***\ndepth        -146.58      35.32  -4.149 3.62e-05 ***\ntable        -118.01      22.05  -5.351 1.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1475 on 996 degrees of freedom\nMultiple R-squared:  0.8679,    Adjusted R-squared:  0.8675 \nF-statistic:  2181 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nRegression Diagnostics\n\nplot(lm_multi, which = 1)\n\n\n\n\n\n\n\n\n\nresid &lt;- resid(lm_multi)\nhist(resid, breaks = 30, main = \"Histogram of Residuals\", xlab = \"Residual\")\n\n\n\n\n\n\n\n\n\n\nANOVA\n\nanova_model &lt;- aov(price ~ cut, data = df_lm)\nsummary(anova_model)\n\n             Df    Sum Sq  Mean Sq F value Pr(&gt;F)  \ncut           4 1.996e+08 49901236   3.065  0.016 *\nResiduals   995 1.620e+10 16283224                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nTukey HSD\nTukeyHSD(anova_model)"
  }
]